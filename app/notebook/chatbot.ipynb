{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import add_messages\n",
    "from ollama import chat\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import re\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic import ValidationError\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatState(TypedDict):\n",
    "    messages : Annotated[list[BaseMessage], add_messages]\n",
    "    # messages: List[dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseSchema(BaseModel):\n",
    "    response : int = Field(gt=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ollama import chat\n",
    "\n",
    "def chat_node(state: ChatState) -> ChatState:\n",
    "    messages = state[\"messages\"]\n",
    "    print(messages)\n",
    "    # Convert BaseMessage ‚Üí Ollama dicts only for the API\n",
    "    messages_dicts = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            messages_dicts.append({\"role\": \"user\", \"content\": m.content})\n",
    "        elif isinstance(m, AIMessage):\n",
    "            messages_dicts.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "\n",
    "    # Call Ollama\n",
    "    response = chat(\n",
    "        model=\"deepseek-r1:1.5b\",\n",
    "        messages=messages_dicts\n",
    "    )\n",
    "\n",
    "    clean_text = re.sub(r\"<think>.*?</think>\", \"\", response[\"message\"][\"content\"], flags=re.DOTALL).strip()\n",
    "    \n",
    "    try:\n",
    "        structured_data = json.loads(clean_text)\n",
    "        validated_output = ResponseSchema(**structured_data)\n",
    "    except (json.JSONDecodeError, ValidationError) as e:\n",
    "        # Fallback if parsing/validation fails\n",
    "        validated_output = ResponseSchema(response=1)  # default/fallback\n",
    "        print(\"Warning: invalid output from model:\", clean_text)\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    # Append assistant response as AIMessage to the state\n",
    "    updated_messages = messages + [AIMessage(content=clean_text)]\n",
    "\n",
    "    return {\"messages\": updated_messages}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(ChatState)\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "\n",
    "graph.add_edge(START, \"chat_node\")\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "\n",
    "workflow = graph.compile(checkpointer= checkpointer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x11c7a7950>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='f1235b8b-71e1-4e6b-9f8b-a61f01d91ab3'), SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='408e2c34-6063-4219-9779-2a0b65d80a17'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c347eb89-4b63-42c9-903e-7d2d840eb697', tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='what is capital of pakistan', additional_kwargs={}, response_metadata={}, id='5296502d-2932-40ba-8c0a-0e8c9bb7d87f'), AIMessage(content='The capital of Pakistan is Quaid-e-Azam University in Islamabad. The city of Islamabad, also known as Quetta, is the largest and most populous city in Pakistan and serves as the administrative center of the country. Quaid-e-Azam University, located on the banks of the Yangtze River, is a prominent institution of higher education in Pakistan.', additional_kwargs={}, response_metadata={}, id='47c1e15e-b7a1-4767-b9f7-87f8d8aed58f', tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='answer 10+ 2', additional_kwargs={}, response_metadata={}, id='984dfaef-40b5-431e-84c7-a8acfb36151f'), AIMessage(content=\"It seems there was a misunderstanding in your last message. However, I can help clarify some facts about Pakistan! The **capitals of Pakistan** are:\\n\\n1. Quaid-e-Azam University ( Islamabad / Quetta )\\n2. Karachi  \\n3. Lahore  \\n4. Fars (Shariaq, Sialak, and Ilkur)  \\n5. Multan  \\n6. Dhtak (Rawalpore)  \\n7. Gujranwala  \\n8. Ilfah (Nizur)  \\n9. Chitral (Taluk Khan)  \\n10. Sialkot  \\n\\nLet me know if you'd like more details!\", additional_kwargs={}, response_metadata={}, id='2e1adbf8-6073-4e22-8b77-a248b753bdd4', tool_calls=[], invalid_tool_calls=[]), SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='7b7db76e-4847-434b-8765-5567f6ebd492')]\n",
      "Warning: invalid output from model: üòä\n",
      "</think>\n",
      "\n",
      "The capital of Pakistan is Quaid-e-Azam University in Islamabad, which is also known as Quetta. The other major cities and places mentioned are significant administrative centers within the country.\n",
      "Error: Expecting value: line 1 column 1 (char 0)\n",
      "[SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='f1235b8b-71e1-4e6b-9f8b-a61f01d91ab3'), SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='408e2c34-6063-4219-9779-2a0b65d80a17'), AIMessage(content='', additional_kwargs={}, response_metadata={}, id='c347eb89-4b63-42c9-903e-7d2d840eb697', tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='what is capital of pakistan', additional_kwargs={}, response_metadata={}, id='5296502d-2932-40ba-8c0a-0e8c9bb7d87f'), AIMessage(content='The capital of Pakistan is Quaid-e-Azam University in Islamabad. The city of Islamabad, also known as Quetta, is the largest and most populous city in Pakistan and serves as the administrative center of the country. Quaid-e-Azam University, located on the banks of the Yangtze River, is a prominent institution of higher education in Pakistan.', additional_kwargs={}, response_metadata={}, id='47c1e15e-b7a1-4767-b9f7-87f8d8aed58f', tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='answer 10+ 2', additional_kwargs={}, response_metadata={}, id='984dfaef-40b5-431e-84c7-a8acfb36151f'), AIMessage(content=\"It seems there was a misunderstanding in your last message. However, I can help clarify some facts about Pakistan! The **capitals of Pakistan** are:\\n\\n1. Quaid-e-Azam University ( Islamabad / Quetta )\\n2. Karachi  \\n3. Lahore  \\n4. Fars (Shariaq, Sialak, and Ilkur)  \\n5. Multan  \\n6. Dhtak (Rawalpore)  \\n7. Gujranwala  \\n8. Ilfah (Nizur)  \\n9. Chitral (Taluk Khan)  \\n10. Sialkot  \\n\\nLet me know if you'd like more details!\", additional_kwargs={}, response_metadata={}, id='2e1adbf8-6073-4e22-8b77-a248b753bdd4', tool_calls=[], invalid_tool_calls=[]), SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='7b7db76e-4847-434b-8765-5567f6ebd492'), AIMessage(content='üòä\\n</think>\\n\\nThe capital of Pakistan is Quaid-e-Azam University in Islamabad, which is also known as Quetta. The other major cities and places mentioned are significant administrative centers within the country.', additional_kwargs={}, response_metadata={}, id='406beef0-7200-4fbe-bce0-d7c30d63f998', tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='answer 10 +2', additional_kwargs={}, response_metadata={}, id='be3b8b3f-9620-4125-ab6d-db6b80c52976')]\n",
      "Warning: invalid output from model: Here are five significant cities related to Pakistan:\n",
      "\n",
      "1. **Karachi**: The capital city of Seringbayev Heights, Pakistan, which is home to the Multan Mir Mir Museum, one of the largest museums in the world.\n",
      "\n",
      "2. **Lahore**: The commercial and financial hub of Pakistan, known for its banking, aerospace industry, and international business sector.\n",
      "\n",
      "If you're looking for more specific details about these cities or other locations, feel free to ask! üòä\n",
      "Error: Expecting value: line 1 column 1 (char 0)\n",
      "Assistant: Here are five significant cities related to Pakistan:\n",
      "\n",
      "1. **Karachi**: The capital city of Seringbayev Heights, Pakistan, which is home to the Multan Mir Mir Museum, one of the largest museums in the world.\n",
      "\n",
      "2. **Lahore**: The commercial and financial hub of Pakistan, known for its banking, aerospace industry, and international business sector.\n",
      "\n",
      "If you're looking for more specific details about these cities or other locations, feel free to ask! üòä\n"
     ]
    }
   ],
   "source": [
    "state: ChatState = {\"messages\": []}\n",
    "thread_id = '1'\n",
    "config = {'configurable' : {'thread_id' : thread_id}}\n",
    "state: ChatState = {\n",
    "    \"messages\": [\n",
    "        SystemMessage(content=\"You are a calculator assistant. Always reply in JSON: {\\\"response\\\": <integer>}\")\n",
    "    ]\n",
    "}\n",
    "state = workflow.invoke(state, config=config)\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Add user message\n",
    "    state[\"messages\"].append(HumanMessage(content=user_input))\n",
    "\n",
    "   \n",
    "    # Run chat node\n",
    "    state = workflow.invoke({'messages': [HumanMessage(content=user_input)]}, config=config)\n",
    "\n",
    "    print(\"Assistant:\", state[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "state = {\"messages\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='01b26529-6553-4922-a20d-ede659573a20'), SystemMessage(content='You are a calculator assistant. Always reply in JSON: {\"response\": <integer>}', additional_kwargs={}, response_metadata={}, id='a9f78d61-7e00-4396-86c7-aea26b9cd1d9')]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ResponseSchema\nreponse\n  Field required [type=missing, input_value={'response': 0}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m config = {\u001b[33m'\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m'\u001b[39m : {\u001b[33m'\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m'\u001b[39m : thread_id}}\n\u001b[32m      4\u001b[39m state: ChatState = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      6\u001b[39m         SystemMessage(content=\u001b[33m\"\u001b[39m\u001b[33mYou are a calculator assistant. Always reply in JSON: \u001b[39m\u001b[33m{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mresponse\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m: <integer>}\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     ]\n\u001b[32m      8\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m state = \u001b[43mworkflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAssistant:\u001b[39m\u001b[33m\"\u001b[39m, state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/langgraph/pregel/main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/langgraph/pregel/main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mchat_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     25\u001b[39m     validated_output = ResponseSchema(**structured_data)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (json.JSONDecodeError, ValidationError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Fallback if parsing/validation fails\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     validated_output = \u001b[43mResponseSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# default/fallback\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: invalid output from model:\u001b[39m\u001b[33m\"\u001b[39m, clean_text)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError:\u001b[39m\u001b[33m\"\u001b[39m, e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODE/Langraphy/Test/patronstaffing_master/myenv/lib/python3.14/site-packages/pydantic/main.py:250\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    249\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    252\u001b[39m     warnings.warn(\n\u001b[32m    253\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    256\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    257\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for ResponseSchema\nreponse\n  Field required [type=missing, input_value={'response': 0}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
      "During task with name 'chat_node' and id '84a1222a-3504-64c4-a9d9-3bf3274e6e9d'"
     ]
    }
   ],
   "source": [
    "thread_id = '1'\n",
    "config = {'configurable' : {'thread_id' : thread_id}}\n",
    "\n",
    "state: ChatState = {\n",
    "    \"messages\": [\n",
    "        SystemMessage(content=\"You are a calculator assistant. Always reply in JSON: {\\\"response\\\": <integer>}\")\n",
    "    ]\n",
    "}\n",
    "state = workflow.invoke(state, config=config)\n",
    "\n",
    "print(\"Assistant:\", state[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is Beijing.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "user_input = \"What is capital city of China\"\n",
    "response = chat(\n",
    "    model='deepseek-r1:1.5b',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': user_input}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "text = response['message']['content']\n",
    "\n",
    "# Remove <think>...</think>\n",
    "clean_text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat started. Type 'exit' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "print(\"Chat started. Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    print(\"Model: \", end=\"\", flush=True)\n",
    "    for chunk in chat(\n",
    "        model=\"deepseek-r1:1.5b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        stream=True,\n",
    "    ):\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The capital of Pakistan is Islamabad.\n",
      "Embedding length: 768\n",
      "First 10 dims: [0.044499103, 0.030817926, -0.1261774, -0.031899728, 0.057978775, 0.06341333, -0.012656392, -0.0019015451, 0.045389988, -0.016721562]\n",
      "---\n",
      "Text: Karachi is the largest city in Pakistan.\n",
      "Embedding length: 768\n",
      "First 10 dims: [0.051595964, 0.05329536, -0.12304631, 0.0041261516, 0.013333, 0.07576696, -0.0065881955, 0.029670743, 0.023827173, 0.0047604693]\n",
      "---\n",
      "Text: Lahore is famous for its culture and food.\n",
      "Embedding length: 768\n",
      "First 10 dims: [0.047556993, 0.06994749, -0.17465745, 0.017788101, -0.0015293012, -0.005974245, -0.048852835, -0.0060100565, 0.02301357, -0.023574123]\n",
      "---\n",
      "Query embedding length: 768\n",
      "First 10 dims: [0.040670663, 0.011320926, -0.1067975, -0.0025148955, 0.05350603, 0.03848292, -0.028616995, -0.004668292, 0.02696397, -0.008000452]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 1Ô∏è‚É£ Setup Ollama embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    base_url=\"http://localhost:11434\"  # make sure Ollama is running\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Example input text\n",
    "texts = [\n",
    "    \"The capital of Pakistan is Islamabad.\",\n",
    "    \"Karachi is the largest city in Pakistan.\",\n",
    "    \"Lahore is famous for its culture and food.\"\n",
    "]\n",
    "\n",
    "# 3Ô∏è‚É£ Generate embeddings\n",
    "embeddings_list = embeddings.embed_documents(texts)\n",
    "\n",
    "# 4Ô∏è‚É£ Print results\n",
    "for text, vec in zip(texts, embeddings_list):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding length: {len(vec)}\")\n",
    "    print(f\"First 10 dims: {vec[:10]}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# 5Ô∏è‚É£ Generate embedding for a single query\n",
    "query = \"What is the capital of Pakistan?\"\n",
    "query_vec = embeddings.embed_query(query)\n",
    "print(\"Query embedding length:\", len(query_vec))\n",
    "print(\"First 10 dims:\", query_vec[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
